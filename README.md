# ğŸš€ RAG with AstraDB, HuggingFace & Groq LLM

A complete **Retrieval-Augmented Generation (RAG)** pipeline that combines:
- ğŸ§© **LangChain** for orchestration  
- ğŸ§  **HuggingFace sentence-transformers** for embeddings  
- ğŸ—„ï¸ **AstraDB Vector Store (DataStax)** for efficient vector search  
- âš¡ **Groq LLM (Llama 3.1-8B)** for fast, context-aware answers  

This project demonstrates an end-to-end **AI knowledge retrieval system** â€” capable of ingesting PDFs, chunking intelligently, storing them as embeddings, and performing semantic Q&A over the content.

---

## ğŸŒŸ Features

âœ… Extracts and cleans text from PDFs using a custom processor  
âœ… Chunks content efficiently with LangChainâ€™s `RecursiveCharacterTextSplitter`  
âœ… Stores document embeddings in **AstraDB Vector Store**  
âœ… Performs **semantic similarity search** and **retrieval-based Q&A**  
âœ… Powered by **Groqâ€™s Llama 3.1-8B model** for low-latency responses    

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|-------|-------------|
| **LLM** | Groq Llama 3.1-8B-Instant |
| **Embeddings** | sentence-transformers/all-MiniLM-L6-v2 |
| **Vector DB** | AstraDB (DataStax) |
| **Framework** | LangChain 0.3+ |
| **Language** | Python 3.10+ |

---
